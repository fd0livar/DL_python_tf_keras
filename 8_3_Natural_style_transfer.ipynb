{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8.3 Natural style transfer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NTOWYDkFfA6ZEuy7Sv0IrTGlDXCfnXDu","authorship_tag":"ABX9TyPyaPSu6HJKzt3RAXRt3cxk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HE9nv7MAh53W","colab_type":"text"},"source":["## Neural style transfer\n","\n","This is another major development in deep-learnig-driven image modifications, introduced by Leon Gatys et al. in the summer of 2015 [[ref]](https://arxiv.org/abs/1508.06576).  This section focuses on the formulation described in the original paper. \n","\n","Neural style transfer consist of applying the style of a reference image to a target image while conserving the contect of the target image. \n","\n","In this context, *style* means textures, colors, and visual patterns in the image, at various spatial scales; and the content is the higher-level macrostructure of the image. \n","\n","This idea has had a long history in the image-processing community prior to the development of neural style transfer in 2015. \n","\n","The key notion behind implementing style transfer is the same idea that's central to all depp-learning algorithms: you define a loss function to specify what you want to achieve, and you minimize this loss. What do we want to achieve? Conserving the content of the original image while adopting the style of the reference image. If we were able to mathematically define *content* and *style*, the an appropriate loss function to minimize would be the following:\n","\n","```python\n","loss = distance(style(reference_image) - style(generated_image)) + \n","       distance(content(original_image) - content(generated_image))\n","```\n","\n","Here `distance` is a norm function such as the L2 norm, `content` is a function that takes an image and computes a representation of its content, and `style` is a function that takes an image and computes a representation of its style. Minimizing this loss causes `style(generated_image)` to be close to `style(reference_image)`, and the same for `content`, thus achieving style transfer as we define it. \n","\n","A very important observation made by Gatys et al. was that deep convolutional neural networks offer a way to mathematically define the `style` and `content` functions. Let's see how:\n","\n","## 8.3.1 The content loss\n","\n","As you already know, activations from earlier layers in a network contain *local* information about the image, whereas activations from higher layers contain increasingly *global*, *abstract* information. Formulated in a different way, the activations of the different layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore, you’d expect the content of an image, which is more global and\n","abstract, to be captured by the representations of the upper layers in a convnet.\n","\n","A good candidate for content loss is thus the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that, as seen from the upper layer, the generated image will look similar to the original target image. Assuming that what the upper layers of a convnet see is really the content of their input images, then this works as a way to preserve image content.\n","\n","\n","## 8.3.2\n","\n","The content loss only uses a single upper layer, but the style loss as defined by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance of the style-reference image at all spatial scales extracted by the convnet, not just a single scale.\n","\n","For the style loss, Gatys et al. use the *Gram matrix* of a layer’s activations: the inner product of the feature maps of a given layer. This inner product can be understood as representing a map of the correlations between the layer’s features. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale.\n","\n","Hence, the style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image.\n","\n","In short, you can use a pretrained convnet to define a loss that will do the following:\n","- Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should “see” both the target image and the generated image as containing the same things.\n","- Preserve style by maintaining similar correlations within activations for both low-\n","level layers and high-level layers. Feature correlations capture textures: the generated image and the style-reference image should share the same textures at different spatial scales.\n","\n","Let's see an implementation of the original 2015 work. It shares many similarities with the DeepDream implementation we already implemented. \n","\n","## 8.3.3 Neural style transfer in Keras\n","\n","We'll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 network intriduced in chapter 5, with 3 more conv. layers.\n","\n","This is the general process:\n","1. Set up a network that computes VGG19 layer activations for the style-reference image, the target image and the generated image at the same time. \n","2. Use the layer activations computed over these three images to define the loss function described earlier, which we'll minimize in order to achieve style transfer.\n","3. Set up a gradient-descent process to minimize this loss function\n","\n","Let's start by defining the paths to the images. \n","\n","### L8.14 Defining initial variables\n"]},{"cell_type":"code","metadata":{"id":"_6mq1ER-WuX4","colab_type":"code","outputId":"210bba88-2474-42e8-cf74-c96fab1a5572","executionInfo":{"status":"ok","timestamp":1588357148926,"user_tz":-120,"elapsed":1004,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ial57jlhWr9c","colab_type":"code","outputId":"84de6a41-1cac-431b-8719-8d2c6296a816","executionInfo":{"status":"ok","timestamp":1588357155091,"user_tz":-120,"elapsed":6906,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":256}},"source":["!pip install keras==2.0.8"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras==2.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/3f/d117d6e48b19fb9589369f4bdbe883aa88943f8bb4a850559ea5c546fefb/Keras-2.0.8-py2.py3-none-any.whl (276kB)\n","\r\u001b[K     |█▏                              | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.12.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.18.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (3.13)\n","\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.0.8 which is incompatible.\u001b[0m\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.3.1\n","    Uninstalling Keras-2.3.1:\n","      Successfully uninstalled Keras-2.3.1\n","Successfully installed keras-2.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"58smntatVx54","colab_type":"code","outputId":"79d24d3d-6093-422e-8b3b-16d76acfed4e","executionInfo":{"status":"ok","timestamp":1588357155096,"user_tz":-120,"elapsed":6609,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd /content/drive/My Drive/kaggle/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/kaggle\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5SAJMYf0W3V6","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"H5Dm6RntVl4d","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import load_img, img_to_array\n","\n","target_image_path = 'm1.jpg'\n","# style_reference_image_path = 'sn_vg.jpg'\n","style_reference_image_path = 'tlor0.jpg'\n","\n","width, height = load_img(target_image_path).size\n","img_height = 400\n","img_width = int(width * img_height / height)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oP4C5Rs9W4xk","colab_type":"text"},"source":["We need to define some functions for loading, preprocessing, and postprocessing the images that go in and out of the VGG19 convnet\n","\n","### L8.15 Auxiliary functions"]},{"cell_type":"code","metadata":{"id":"E8PvbNQkXNwr","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras.applications import vgg19\n","\n","def preprocess_image(image_path):\n","  img = load_img(image_path, target_size=(img_height, img_width))\n","  img = img_to_array(img)\n","  img = np.expand_dims(img, axis=0)\n","  img = vgg19.preprocess_input(img)\n","  return img\n","\n","\n","# zero-centering by removing the mean pixel value from ImageNet. This reverses a\n","# transformation done by vgg19.preprocess_input\n","def deprocess_image(x):\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","  # converts images from 'BGR' to 'RGB'. This is also part of the reversal of \n","  # vgg19.preprocess_input\n","  x = x[:, :, ::-1] \n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Br05EOhHX-0e","colab_type":"text"},"source":["Let’s set up the VGG19 network. It takes as input a batch of three images: the style-reference image, the target image, and a placeholder that will contain the generated image. A placeholder is a symbolic tensor, the values of which are provided externally via Numpy arrays. The style-reference and target image are static and thus defined using K.constant , whereas the values contained in the placeholder of the generated image will change over time.\n","\n","### L8.16 Loading the pretrained VGG19 network and applying it to the three images"]},{"cell_type":"code","metadata":{"id":"SdFwB16UYJPp","colab_type":"code","outputId":"3a0ad41a-6c0e-404e-c418-08b21c2e38ab","executionInfo":{"status":"ok","timestamp":1588359624419,"user_tz":-120,"elapsed":3358,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from keras import backend as K\n","\n","target_image = K.constant(preprocess_image(target_image_path))\n","\n","style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n","\n","combination_image = K.placeholder((1, img_height, img_width, 3))\n","\n","# combining the 3 images in a single batch\n","input_tensor = K.concatenate([target_image, \n","                              style_reference_image, \n","                              combination_image], \n","                             axis=0)\n","\n","model = vgg19.VGG19(input_tensor=input_tensor, \n","                    weights='imagenet',\n","                    include_top=False)\n","\n","print('Model loaded.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model loaded.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40EYXx9KZEQP","colab_type":"text"},"source":["Let's define the content loss, which will make sure the top layer of the VGG19 convnet has a similar view of the target image and the generated image. \n","\n","### L8.17 Content loss"]},{"cell_type":"code","metadata":{"id":"h-wJQ9UcZSGn","colab_type":"code","colab":{}},"source":["def content_loss(base, combination):\n","  return K.sum(K.square(combination - base))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKCWmbFKZa_t","colab_type":"text"},"source":["Now the style loss. It uses an aux function to compute the Gram matrix of an input matrix: a map of the correlations found in the original feature matrix.\n","\n","### L8.18 Style loss"]},{"cell_type":"code","metadata":{"id":"n5GBKJ4YZsYb","colab_type":"code","colab":{}},"source":["def gram_matrix(x):\n","  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n","  gram = K.dot(features, K.transpose(features))\n","  return gram\n","\n","def style_loss(style, combination):\n","  S = gram_matrix(style)\n","  C = gram_matrix(combination)\n","  channels = 3\n","  size = img_height * img_width\n","  return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ctjb5SlaND3","colab_type":"text"},"source":["To these 2 loss components we add a third one: the *total variation loss*, which operates on the pixels of the generated combination image. It encourages spatial continuity in the generated image, thus avoiding overly pixelated results. You can interpret it as a regularization loss. \n","\n","### L8.19 Total variation loss"]},{"cell_type":"code","metadata":{"id":"ICA-MzAbanAn","colab_type":"code","colab":{}},"source":["def total_variation_loss(x):\n","  a = K.square(\n","      x[:, :img_height - 1, :img_width - 1, :] - \n","      x[:, 1:, :img_width - 1, :])\n","\n","  b = K.square(\n","      x[:, :img_height - 1, :img_width - 1, :] - \n","      x[:, :img_height - 1, 1:, :])\n","\n","  return K.sum(K.pow(a + b, 1.25))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTIpiBfza0Cm","colab_type":"text"},"source":["We will minimize a weighted average of these three losses. \n","\n","To compute the content loss, we use only one upper layer (the `block5_conv2` layer) whereas for the\n","style loss, we use a list of layers than spans both low-level and high-level layers. \n","\n","We add the total variation loss at the end.\n","\n","We can tune the `content_weight` parameter.\n","A higher `content_weight` means the target content will be more recognizable in the generated image.\n","\n","### L8.20 Defining the final loss that you'll minimize"]},{"cell_type":"code","metadata":{"id":"7fEs8apubkxi","colab_type":"code","outputId":"62b79620-ab33-4355-f855-d601261ecf86","executionInfo":{"status":"ok","timestamp":1588359629928,"user_tz":-120,"elapsed":1347,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n","\n","content_layer = 'block5_conv2'\n","\n","style_layers = ['block1_conv1', \n","                'block2_conv1',\n","                'block3_conv1',\n","                'block4_conv1',\n","                'block5_conv1']\n","\n","\n","total_variation_weight = 1e-4\n","style_weight = 1.\n","content_weight = 0.025\n","\n","loss = K.variable(0.)\n","layer_features = outputs_dict[content_layer]\n","target_image_features = layer_features[0, :, :, :]\n","combination_features = layer_features[2, :, :, :]\n","loss += content_weight * content_loss(target_image_features, combination_features)\n","\n","for layer_name in style_layers:\n","  layer_features = outputs_dict[layer_name]\n","  style_reference_features = layer_features[1, :, :, :]\n","  combination_features = layer_features[2, :, :, :]\n","  sl = style_loss(style_reference_features, combination_features)\n","  loss += (style_weight / len(style_layers)) * sl\n","\n","loss += total_variation_weight * total_variation_loss(combination_image)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-SJmsshjb5rX","colab_type":"text"},"source":["Now we’ll set up the gradient-descent process. In the original Gatys et al. paper,\n","optimization is performed using the L-BFGS algorithm, so that’s what you’ll use here. This is a key difference from the DeepDream example in section 8.2. The L-BFGS algorithm comes packaged with SciPy, but there are two slight limitations with the SciPy implementation:\n","- It requires that you pass the value of the loss function and the value of the gra-\n","dients as two separate functions.\n","- It can only be applied to flat vectors, whereas you have a 3D image array.\n","\n","It would be inefficient to compute the value of the loss function and the value of the gradients independently, because doing so would lead to a lot of redundant computation between the two; the process would be almost twice as slow as computing them jointly. To bypass this, you’ll set up a Python class named Evaluator that computes both the loss value and the gradients value at once, returns the loss value when called the first time, and caches the gradients for the next call.\n","\n","### L8.21 Setting up the gradient-descent process"]},{"cell_type":"code","metadata":{"id":"2MHoWfL1cdsZ","colab_type":"code","colab":{}},"source":["grads = K.gradients(loss, combination_image)[0]\n","\n","fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n","\n","class Evaluator(object):\n","  \n","  def __init__(self):\n","    self.loss_value = None\n","    self.grads_values = None\n","\n","  def loss(self, x):\n","    assert self.loss_value is None\n","    x = x.reshape((1, img_height, img_width, 3))\n","    outs = fetch_loss_and_grads([x])\n","    loss_value = outs[0]\n","    grad_values = outs[1].flatten().astype('float64')\n","    self.loss_value = loss_value\n","    self.grad_values = grad_values\n","    return self.loss_value\n","\n","  def grads(self, x):\n","    assert self.loss_value is not None\n","    grad_values = np.copy(self.grad_values)\n","    self.loss_value = None\n","    self.grad_values = None\n","    return grad_values\n","\n","evaluator = Evaluator()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJk2v-dRdJPT","colab_type":"text"},"source":["Finally, you can run the gradient-ascent process using SciPy’s L-BFGS algorithm, saving the current generated image at each iteration of the algorithm (here, a single iteration represents 20 steps of gradient ascent).\n","\n","### L8.22 Style-transfer loop"]},{"cell_type":"code","metadata":{"id":"fv2yQjyIdV0h","colab_type":"code","outputId":"3ccf4ada-4af0-4d47-c904-124967fdf192","executionInfo":{"status":"ok","timestamp":1588359949167,"user_tz":-120,"elapsed":318466,"user":{"displayName":"Fa Oli","photoUrl":"","userId":"10660454401649634141"}},"colab":{"base_uri":"https://localhost:8080/","height":752}},"source":["from scipy.optimize import fmin_l_bfgs_b\n","# from scipy.misc import imsave\n","import imageio\n","import time\n","\n","result_prefix = 'my_result'\n","iterations = 20\n","\n","x = preprocess_image(target_image_path)\n","x = x.flatten()\n","\n","for i in range(iterations):\n","  print('Start of iteration', i)\n","  start_time = time.time()\n","  x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n","                                   x,\n","                                   fprime=evaluator.grads,\n","                                   maxfun=20)\n","  \n","  print('Current loss value:', min_val)\n","  img = x.copy().reshape((img_height, img_width, 3))\n","  img = deprocess_image(img)\n","  fname = result_prefix + '_at_iteration_%d.png' % i\n","  # imsave(fname, img)\n","  imageio.imwrite(fname, img)\n","  # print('Image saved as', fname)\n","  end_time = time.time()\n","  # print('Iteration %d completed in %ds' % (i, end_time - start_time))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start of iteration 0\n","Current loss value: 271311070.0\n","Start of iteration 1\n","Current loss value: 146308420.0\n","Start of iteration 2\n","Current loss value: 113658760.0\n","Start of iteration 3\n","Current loss value: 97613230.0\n","Start of iteration 4\n","Current loss value: 86635784.0\n","Start of iteration 5\n","Current loss value: 78297190.0\n","Start of iteration 6\n","Current loss value: 72437260.0\n","Start of iteration 7\n","Current loss value: 67907240.0\n","Start of iteration 8\n","Current loss value: 64273824.0\n","Start of iteration 9\n","Current loss value: 61370540.0\n","Start of iteration 10\n","Current loss value: 58472676.0\n","Start of iteration 11\n","Current loss value: 56075296.0\n","Start of iteration 12\n","Current loss value: 53882150.0\n","Start of iteration 13\n","Current loss value: 52020732.0\n","Start of iteration 14\n","Current loss value: 50177236.0\n","Start of iteration 15\n","Current loss value: 48594600.0\n","Start of iteration 16\n","Current loss value: 47076148.0\n","Start of iteration 17\n","Current loss value: 45704596.0\n","Start of iteration 18\n","Current loss value: 44278616.0\n","Start of iteration 19\n","Current loss value: 43098984.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o6qAAO0Gd-1x","colab_type":"text"},"source":["Note that running this style-transfer algorithm is slow. But the transformation operated by the setup is simple enough that it can be learned by a small, fast\n","feedforward convnet as well—as long as you have appropriate training data available.\n","\n","Fast style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style-reference image, using the method outlined here, and then training a simple convnet to learn this style-specific transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a forward pass of this small convnet\n","\n","## 8.3.4 Wrapping up\n","\n","- Style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image.\n","- Content can be captured by the high-level activations of a convnet.\n","- Style can be captured by the internal correlations of the activations of different layers of a convnet.\n","- Hence, deep learning allows style transfer to be formulated as an optimization\n","process using a loss defined with a pretrained convnet.\n","- Starting from this basic idea, many variants and refinements are possible."]},{"cell_type":"code","metadata":{"id":"au80c8zCn6re","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}