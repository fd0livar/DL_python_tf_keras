{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Advanced use of RNN\n\nThere are 3 advances techniques for improving the performance and generalization power of RNNs. \n\n- Recurrent dropout\n- Stacking recurrent layers\n- Bidirectional recurrent layers"},{"metadata":{},"cell_type":"markdown","source":"### A temperature forecasting problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndata_dir = '/kaggle/input/weather-archive-jena/'\nfname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n\nf = open(fname)\ndata = f.read()\nf.close()\n\nlines = data.split('\\n')\nheader = lines[0].split(',')\nlines = lines[1:]\n\nprint(header)\nprint(len(lines))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 420551 lines of data. We should convert them to numpy arrays so we can work with them."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nfloat_data = np.zeros((len(lines), len(header) - 1))\nfor i, line in enumerate(lines):\n    values = [float(x) for x in line.split(',')[1:]]\n    float_data[i, :] = values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a numpy array (matrix) organizes as `lines x values`  \nWe can plot the temperature:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntemp = float_data[:, 1] \nplt.plot(range(len(temp)), temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as each datapoint was collected every 10 minutes, \n# we can plot the temp data of the first 10 days of the recorded data\n\nplt.plot(range(1440), temp[:1440])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see the yearly periodicity of the temp, which is not there if we have a look at a few days timespan. \n\nWe want to predict the temperature 24 hours in the future for a given time window, let's try that with NNs. \n\n### Preparing the data\n\nFirst, we normalize the data, only using the first 200000 timestamps as training data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = float_data[:200000].mean(axis=0)\nfloat_data -= mean\nstd = float_data[:200000].std(axis=0)\nfloat_data /= std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we create a generator that returns the timeseries samples and an array of targets temperatures"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n    '''\n    - data : The original array of floating-point data, which you normalized in listing 6.32.\n    - lookback : How many timesteps back the input data should go.\n    - delay : How many timesteps in the future the target should be.\n    - min_index and max_index : Indices in the data array that delimit which timesteps to draw from. \n                                This is useful for keeping a segment of the data for validation and \n                                another for testing.\n    - shuffle : Whether to shuffle the samples or draw them in chronological order.\n    - batch_size : The number of samples per batch.\n    - step : The period, in timesteps, at which you sample data. You’ll set it to 6 in order to draw \n            one data point every hour.\n    '''\n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n        else:\n            if i + batch_size >= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n            \n        samples = np.zeros((len(rows), \n                            lookback // step, \n                            data.shape[-1]))\n        \n        targets = np.zeros((len(rows), ))\n        \n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n            \n        yield samples, targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the generator function to instantiate 3 generators: one for training, one for validation, and one for testing. Each one will take different temporal segments of the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"lookback = 1440\nstep = 6\ndelay = 144\nbatch_size = 128\n\ntrain_gen = generator(float_data, \n                      lookback=lookback, \n                      delay=delay, \n                      min_index=0, \n                      max_index=200000, \n                      shuffle=True, \n                      step=step, \n                      batch_size=batch_size)\n\nval_gen = generator(float_data, \n                    lookback=lookback, \n                    delay=delay, \n                    min_index=200001, \n                    max_index=300000, \n                    step=step, \n                    batch_size=batch_size)\n\ntest_gen = generator(float_data, \n                     lookback=lookback, \n                     delay=delay, \n                     min_index=300001, \n                     max_index=None, \n                     step=step, \n                     batch_size=batch_size)\n\nval_steps = (300000 - 200001 - lookback) // batch_size\n\ntest_steps = (len(float_data) - 300001 - lookback) // batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to have a baseline method to which our network method is compared, we create a naive method that predicts the temperature 24 hours from now to be exactly the same as right now. We evaluate this method with the mean absolute error (MAE):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_naive_method():\n    batch_maes = [] \n    for step in range(val_steps): \n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets)) \n        batch_maes.append(mae)\n    print(np.mean(batch_maes))\n    \nevaluate_naive_method()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"celsius_mae = 0.29 * std[1]\nprint(celsius_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an average absolute error of about `2.57ºC` as baseline"},{"metadata":{},"cell_type":"markdown","source":"### A basic Machine-learning approach\n\nBefore using RNN, let's use Dense layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport time\n\nmodel = Sequential()\nmodel.add(layers.Flatten(input_shape=(lookback // step, \n                                      float_data.shape[-1])))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1))\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\nstart = time.perf_counter()\n\nhistory = model.fit_generator(train_gen, \n                              steps_per_epoch=500, \n                              epochs=20, \n                              validation_data=val_gen, \n                              validation_steps=val_steps)\n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"local GPU time: 351.782 seconds -> 5.86 minutes  \nkaggle GPU time: 400.122 second -> 6.67 minutes\n\nNow we plot the loss curves:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r*', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can compare the validation loss from this plot to the MAE value obtained from the baseline calculation (~0.29). We are at about the same value using this simple neural network. Our informed assumption is very powerful in comparison to this simple machine learning approach. \n\n### First recurrent baseline\n\nNow, by using an RNN we will be reading the data as a sequence, which is perfect for this kind of data. \n\nWe'll be using the Gate Recurrent Unit ([GRU](https://arxiv.org/abs/1412.3555)) layer, which follows the same principle as LSTM but it's cheaper to run. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport time\n\nmodel = Sequential()\nmodel.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\nmodel.add(layers.Dense(1))\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\nstart = time.perf_counter()\n\nhistory = model.fit_generator(train_gen, \n                              steps_per_epoch=500, \n                              epochs=20, \n                              validation_data=val_gen, \n                              validation_steps=val_steps)\n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Local GPU time: 6552.676 seconds -> 1.8 hrs!  \nKaggle GPU time: 3694.019 seconds -> 61.5 minutes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\n\n# models.save_model(model, 'first_rnn_jena_climate.h5')\nmodels.save_model(model, '/kaggle/working/first_rnn_jena_climate_k.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r*', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the validation loss reached a lower value than the baseline. "},{"metadata":{"trusted":true},"cell_type":"code","source":"min(val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"celsius_mae_simple_rnn = min(val_loss) * std[1]\nprint('Mean absolute error = {:.3}ºC'.format(celsius_mae_simple_rnn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a MAE of about `2.32ºC`, better than the baseline of `2.57ºC`.\n\n### Using recurrent dropout to fight overfitting\n\nIn the case of recurrent layers, there is another type of dropout usage, the recurrent dropout, that applies the same dropout to the recurrent activation of the layer. This method is the best for RNN according to this [thesis](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras  import layers\nfrom keras.optimizers import RMSprop\nimport time \n\nmodel = Sequential()\nmodel.add(layers.GRU(32, \n                     dropout=0.2, \n                     recurrent_dropout=0.2, \n                     input_shape=(None, float_data.shape[-1])))\nmodel.add(layers.Dense(1))\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n# start = time.perf_counter()\n\nhistory = model.fit_generator(train_gen, \n                              steps_per_epoch=500, \n                              epochs=2, \n                              validation_data=val_gen, \n                              validation_steps=val_steps)\n\n# elapsed = time.perf_counter() - start\n# print('Elapsed %.3f seconds.' % elapsed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r*', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"nbformat":4,"nbformat_minor":4}